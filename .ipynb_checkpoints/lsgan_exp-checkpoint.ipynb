{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(Diters=5, adam=False, batchSize=128, beta1=0.5, clamp_lower=-0.01, clamp_upper=0.01, cuda=True, experiment=None, lrD=5e-05, lrG=5e-05, mlp_D=True, mlp_G=True, nSize=148, n_extra_layers=0, ndf=64, neg_data='/storage03/user_data/liuchen01/creds/train_pos.dat', netD='', netG='', netP='samples/netD_epoch_24.pth', ngf=64, ngpu=1, niter=25, noBN=False, nz=148, pos_data='/storage03/user_data/liuchen01/creds/train_neg.dat', test_data='/storage03/user_data/liuchen01/creds/test_feature.dat', test_label='/storage03/user_data/liuchen01/creds/test_labels.dat', workers=2)\n",
      "Random Seed:  9249\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from dataset import DatasetFromPandas\n",
    "import models.dcgan as dcgan\n",
    "import models.mlp as mlp\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--pos_data', default='/storage03/user_data/liuchen01/creds/train_neg.dat', help='path to dataset')\n",
    "parser.add_argument('--neg_data', default='/storage03/user_data/liuchen01/creds/train_pos.dat', help='path to dataset')\n",
    "parser.add_argument('--test_data', default='/storage03/user_data/liuchen01/creds/test_feature.dat', help='path to dataset')\n",
    "parser.add_argument('--test_label', default='/storage03/user_data/liuchen01/creds/test_labels.dat', help='path to dataset')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=2)\n",
    "parser.add_argument('--batchSize', type=int, default=128, help='input batch size')\n",
    "parser.add_argument('--nSize', type=int, default=148, help='noise size')\n",
    "parser.add_argument('--nz', type=int, default=148, help='size of the latent z vector')\n",
    "parser.add_argument('--ngf', type=int, default=64)\n",
    "parser.add_argument('--ndf', type=int, default=64)\n",
    "parser.add_argument('--niter', type=int, default=25, help='number of epochs to train for')\n",
    "parser.add_argument('--lrD', type=float, default=0.00005, help='learning rate for Critic, default=0.00005')\n",
    "parser.add_argument('--lrG', type=float, default=0.00005, help='learning rate for Generator, default=0.00005')\n",
    "parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for adam. default=0.5')\n",
    "parser.add_argument('--cuda'  , action='store_true',default=True, help='enables cuda')\n",
    "parser.add_argument('--ngpu'  , type=int, default=1, help='number of GPUs to use')\n",
    "parser.add_argument('--netG', default='', help=\"path to netG (to continue training)\")\n",
    "parser.add_argument('--netD', default='', help=\"path to netD (to continue training)\")\n",
    "parser.add_argument('--netP', default='samples/netD_epoch_24.pth', help=\"path to netP (to continue training)\")\n",
    "parser.add_argument('--clamp_lower', type=float, default=-0.01)\n",
    "parser.add_argument('--clamp_upper', type=float, default=0.01)\n",
    "parser.add_argument('--Diters', type=int, default=5, help='number of D iters per each G iter')\n",
    "parser.add_argument('--noBN', action='store_true', help='use batchnorm or not (only for DCGAN)')\n",
    "parser.add_argument('--mlp_G', action='store_true',default=True, help='use MLP for G')\n",
    "parser.add_argument('--mlp_D', action='store_true', default=True,help='use MLP for D')\n",
    "parser.add_argument('--n_extra_layers', type=int, default=0, help='Number of extra layers on gen and disc')\n",
    "parser.add_argument('--experiment', default=None, help='Where to store samples and models')\n",
    "parser.add_argument('--adam', action='store_true', help='Whether to use adam (default is rmsprop)')\n",
    "opt, unknown = parser.parse_known_args()\n",
    "print(opt)\n",
    "\n",
    "\n",
    "if opt.experiment is None:\n",
    "    opt.experiment = 'samples'\n",
    "os.system('mkdir {0}'.format(opt.experiment))\n",
    "\n",
    "opt.manualSeed = random.randint(1, 10000) # fix seed\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_data = DatasetFromPandas(opt.pos_data)\n",
    "\n",
    "neg_data = DatasetFromPandas(opt.neg_data)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(pos_data, batch_size=opt.batchSize,\n",
    "                                         shuffle=True, num_workers=int(opt.workers))\n",
    "neg_dataloader = torch.utils.data.DataLoader(neg_data, batch_size=opt.batchSize,\n",
    "                                         shuffle=True, num_workers=int(opt.workers))\n",
    "test = DatasetFromPandas(opt.test_data)\n",
    "labels = list(pd.read_csv(opt.test_label,header=None)[0])\n",
    "testdataloader = torch.utils.data.DataLoader(test, batch_size=len(test),\n",
    "                                         shuffle=False, num_workers=int(opt.workers))\n",
    "testdataiter = iter(testdataloader)\n",
    "testv = Variable(testdataiter.next()).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential (\n",
      "  (0): Linear (148 -> 64)\n",
      "  (1): ReLU (inplace)\n",
      "  (2): Linear (64 -> 64)\n",
      "  (3): ReLU (inplace)\n",
      "  (4): Linear (64 -> 64)\n",
      "  (5): ReLU (inplace)\n",
      "  (6): Linear (64 -> 148)\n",
      "  (7): Sigmoid ()\n",
      ")\n",
      "Sequential (\n",
      "  (0): Linear (148 -> 64)\n",
      "  (1): ReLU (inplace)\n",
      "  (2): Linear (64 -> 64)\n",
      "  (3): ReLU (inplace)\n",
      "  (4): Linear (64 -> 64)\n",
      "  (5): ReLU (inplace)\n",
      "  (6): Linear (64 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ngpu = int(opt.ngpu)\n",
    "nSize = int(opt.nz)\n",
    "\n",
    "nz = int(opt.nz)\n",
    "nSize = int (opt.nSize)\n",
    "ngf = int(opt.ngf)\n",
    "ndf = int(opt.ndf)\n",
    "n_extra_layers = int(opt.n_extra_layers)\n",
    "\n",
    "netG = nn.Sequential(\n",
    "    # Z goes into a linear of size: ngf\n",
    "    nn.Linear(nz, ngf),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(ngf, ngf),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(ngf, ngf),\n",
    "    nn.ReLU(True),\n",
    "    nn.Linear(ngf, nSize),\n",
    "    nn.Sigmoid()\n",
    "\n",
    ")\n",
    "\n",
    "netD = nn.Sequential(\n",
    "        nn.Linear(nSize, ndf),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(ndf, ndf),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(ndf, ndf),\n",
    "        nn.ReLU(True),\n",
    "        nn.Linear(ndf, 1)\n",
    ")\n",
    "\n",
    "print (netG)\n",
    "print (netD)\n",
    "input = torch.FloatTensor(opt.batchSize, opt.nSize)\n",
    "noise = torch.FloatTensor(opt.batchSize, nz)\n",
    "# fixed_noise = torch.FloatTensor(opt.batchSize, nz, 1, 1).normal_(0, 1)\n",
    "\n",
    "if opt.cuda:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    input = input.cuda()\n",
    "    noise = noise.cuda()\n",
    "\n",
    "# setup optimizer\n",
    "if opt.adam:\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=opt.lrD, betas=(opt.beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=opt.lrG, betas=(opt.beta1, 0.999))\n",
    "else:\n",
    "    optimizerD = optim.RMSprop(netD.parameters(), lr = opt.lrD)\n",
    "    optimizerG = optim.RMSprop(netG.parameters(), lr = opt.lrG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = torch.FloatTensor(opt.batchSize, opt.nSize)\n",
    "\n",
    "\n",
    "if opt.cuda:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    input = input.cuda()\n",
    "    # noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "# setup optimizer\n",
    "if opt.adam:\n",
    "    optimizerD = optim.Adam(netD.parameters(), lr=opt.lrD, betas=(opt.beta1, 0.999))\n",
    "    optimizerG = optim.Adam(netG.parameters(), lr=opt.lrG, betas=(opt.beta1, 0.999))\n",
    "else:\n",
    "    optimizerD = optim.RMSprop(netD.parameters(), lr = opt.lrD)\n",
    "    optimizerG = optim.RMSprop(netG.parameters(), lr = opt.lrG)\n",
    "\n",
    "test = DatasetFromPandas(opt.test_data)\n",
    "labels = list(pd.read_csv(opt.test_label,header=None)[0])\n",
    "def reset_grad():\n",
    "    netG.zero_grad()\n",
    "    netD.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][5/1662] Loss_D: 0.628917 Loss_G: 0.610509 \n",
      "[-0.09870499] [-0.13097212]\n",
      "[-0.10483804] [-0.10516929]\n",
      "[-0.0840505] [-0.14296287] 205608\n",
      "[-0.10461623] [-0.10513104] 205608\n",
      "[0/25][10/1662] Loss_D: 0.604769 Loss_G: 0.583747 \n",
      "[-0.07816623] [-0.11164208]\n",
      "[-0.08032493] [-0.08065491]\n",
      "[-0.06153231] [-0.12535223] 205608\n",
      "[-0.08009847] [-0.0806626] 205608\n",
      "[0/25][15/1662] Loss_D: 0.584756 Loss_G: 0.555390 \n",
      "[-0.0593017] [-0.09912963]\n",
      "[-0.0537451] [-0.05412465]\n",
      "[-0.04275929] [-0.11186636] 205608\n",
      "[-0.05341022] [-0.05414418] 205608\n",
      "[0/25][20/1662] Loss_D: 0.567428 Loss_G: 0.528233 \n",
      "[-0.04144558] [-0.08513905]\n",
      "[-0.02759267] [-0.02804783]\n",
      "[-0.02495361] [-0.1007928] 205608\n",
      "[-0.0274021] [-0.028051] 205608\n",
      "[0/25][25/1662] Loss_D: 0.552476 Loss_G: 0.507203 \n",
      "[-0.01263861] [-0.07751261]\n",
      "[-0.00692897] [-0.00735085]\n",
      "[-0.00730195] [-0.09078789] 205608\n",
      "[-0.00666007] [-0.00742303] 205608\n",
      "[0/25][30/1662] Loss_D: 0.537858 Loss_G: 0.488647 \n",
      "[-0.00632624] [-0.06456392]\n",
      "[ 0.01170149] [ 0.01119357]\n",
      "[ 0.01200913] [-0.08180332] 205608\n",
      "[ 0.01207092] [ 0.01109098] 205608\n",
      "[0/25][35/1662] Loss_D: 0.524233 Loss_G: 0.471783 \n",
      "[ 0.00391109] [-0.05186677]\n",
      "[ 0.02894603] [ 0.02838121]\n",
      "[ 0.03156517] [-0.07234298] 205608\n",
      "[ 0.02953335] [ 0.02834482] 205608\n",
      "[0/25][40/1662] Loss_D: 0.511507 Loss_G: 0.457200 \n",
      "[ 0.02811814] [-0.04538209]\n",
      "[ 0.04413281] [ 0.04338596]\n",
      "[ 0.04959176] [-0.06303696] 205608\n",
      "[ 0.04468098] [ 0.04338834] 205608\n",
      "[0/25][45/1662] Loss_D: 0.492317 Loss_G: 0.442829 \n",
      "[ 0.04325899] [-0.03076575]\n",
      "[ 0.05944481] [ 0.05855747]\n",
      "[ 0.0703102] [-0.0553638] 205608\n",
      "[ 0.06003895] [ 0.05849002] 205608\n",
      "[0/25][50/1662] Loss_D: 0.478640 Loss_G: 0.429209 \n",
      "[ 0.0692187] [-0.01524962]\n",
      "[ 0.07415917] [ 0.0730777]\n",
      "[ 0.09311498] [-0.04766184] 205608\n",
      "[ 0.07482331] [ 0.07303017] 205608\n",
      "[0/25][55/1662] Loss_D: 0.463038 Loss_G: 0.418018 \n",
      "[ 0.09959373] [ 0.00756661]\n",
      "[ 0.08663841] [ 0.08508364]\n",
      "[ 0.11560462] [-0.03963125] 205608\n",
      "[ 0.08696147] [ 0.08509962] 205608\n",
      "[0/25][60/1662] Loss_D: 0.448860 Loss_G: 0.406767 \n",
      "[ 0.12847674] [ 0.00878616]\n",
      "[ 0.09917017] [ 0.09747171]\n",
      "[ 0.13807744] [-0.03209243] 205608\n",
      "[ 0.09971162] [ 0.09749121] 205608\n",
      "[0/25][65/1662] Loss_D: 0.433827 Loss_G: 0.396749 \n",
      "[ 0.13958976] [ 0.02334812]\n",
      "[ 0.10971578] [ 0.10866297]\n",
      "[ 0.16109896] [-0.02503271] 205608\n",
      "[ 0.11100275] [ 0.10848217] 205608\n",
      "[0/25][70/1662] Loss_D: 0.413536 Loss_G: 0.388750 \n",
      "[ 0.1626929] [ 0.04932339]\n",
      "[ 0.11945423] [ 0.11747067]\n",
      "[ 0.18513924] [-0.01800565] 205608\n",
      "[ 0.12032329] [ 0.11742681] 205608\n",
      "[0/25][75/1662] Loss_D: 0.398245 Loss_G: 0.380654 \n",
      "[ 0.18192342] [ 0.0507636]\n",
      "[ 0.12839329] [ 0.12649611]\n",
      "[ 0.21183991] [-0.01069316] 205608\n",
      "[ 0.12958404] [ 0.12645382] 205608\n",
      "[0/25][80/1662] Loss_D: 0.381797 Loss_G: 0.374795 \n",
      "[ 0.20642461] [ 0.06772913]\n",
      "[ 0.13527668] [ 0.13352185]\n",
      "[ 0.23907194] [-0.00300856] 205608\n",
      "[ 0.13660842] [ 0.13330179] 205608\n",
      "[0/25][85/1662] Loss_D: 0.365431 Loss_G: 0.368805 \n",
      "[ 0.23429605] [ 0.07938132]\n",
      "[ 0.14334938] [ 0.14022264]\n",
      "[ 0.26727387] [ 0.00474671] 205608\n",
      "[ 0.14402291] [ 0.13996583] 205608\n",
      "[0/25][90/1662] Loss_D: 0.346090 Loss_G: 0.363636 \n",
      "[ 0.2460264] [ 0.11436795]\n",
      "[ 0.14831017] [ 0.14623462]\n",
      "[ 0.29755941] [ 0.01277734] 205608\n",
      "[ 0.14996633] [ 0.14615448] 205608\n",
      "[0/25][95/1662] Loss_D: 0.327200 Loss_G: 0.359357 \n",
      "[ 0.29019943] [ 0.14205545]\n",
      "[ 0.15408999] [ 0.15112045]\n",
      "[ 0.32891911] [ 0.0203969] 205608\n",
      "[ 0.155754] [ 0.15083832] 205608\n",
      "[0/25][100/1662] Loss_D: 0.309110 Loss_G: 0.356989 \n",
      "[ 0.32990032] [ 0.15953174]\n",
      "[ 0.15680996] [ 0.15399799]\n",
      "[ 0.36042383] [ 0.02863645] 205608\n",
      "[ 0.15874211] [ 0.15370408] 205608\n",
      "[0/25][105/1662] Loss_D: 0.289508 Loss_G: 0.355750 \n",
      "[ 0.34803101] [ 0.16633731]\n",
      "[ 0.15827629] [ 0.15507266]\n",
      "[ 0.39220527] [ 0.03701437] 205608\n",
      "[ 0.16056734] [ 0.15482494] 205608\n",
      "[0/25][110/1662] Loss_D: 0.271908 Loss_G: 0.356313 \n",
      "[ 0.36426094] [ 0.17162326]\n",
      "[ 0.1576212] [ 0.15447903]\n",
      "[ 0.4252317] [ 0.04496149] 205608\n",
      "[ 0.16081041] [ 0.15390253] 205608\n",
      "[0/25][115/1662] Loss_D: 0.247941 Loss_G: 0.357197 \n",
      "[ 0.4296906] [ 0.22462694]\n",
      "[ 0.15737098] [ 0.15304121]\n",
      "[ 0.45865738] [ 0.05356625] 205608\n",
      "[ 0.16020234] [ 0.15264513] 205608\n",
      "[0/25][120/1662] Loss_D: 0.235787 Loss_G: 0.359699 \n",
      "[ 0.4472661] [ 0.16823763]\n",
      "[ 0.15511729] [ 0.15011133]\n",
      "[ 0.49304801] [ 0.06222226] 205608\n",
      "[ 0.15782756] [ 0.14939731] 205608\n",
      "[0/25][125/1662] Loss_D: 0.223688 Loss_G: 0.362613 \n",
      "[ 0.45551494] [ 0.22635064]\n",
      "[ 0.15125251] [ 0.14634082]\n",
      "[ 0.52767819] [ 0.07079124] 205608\n",
      "[ 0.15484107] [ 0.14569387] 205608\n",
      "[0/25][130/1662] Loss_D: 0.200139 Loss_G: 0.366416 \n",
      "[ 0.48117027] [ 0.22998407]\n",
      "[ 0.14710364] [ 0.14155485]\n",
      "[ 0.5624547] [ 0.07946698] 205608\n",
      "[ 0.15122193] [ 0.14109714] 205608\n",
      "[0/25][135/1662] Loss_D: 0.177229 Loss_G: 0.371180 \n",
      "[ 0.55642992] [ 0.28555784]\n",
      "[ 0.14273112] [ 0.13656144]\n",
      "[ 0.5976603] [ 0.08790837] 205608\n",
      "[ 0.14634068] [ 0.13537791] 205608\n",
      "[0/25][140/1662] Loss_D: 0.166581 Loss_G: 0.374853 \n",
      "[ 0.5968644] [ 0.23886287]\n",
      "[ 0.13817239] [ 0.13161877]\n",
      "[ 0.63303423] [ 0.09689654] 205608\n",
      "[ 0.14340769] [ 0.13078527] 205608\n",
      "[0/25][145/1662] Loss_D: 0.156227 Loss_G: 0.379590 \n",
      "[ 0.61817414] [ 0.26671168]\n",
      "[ 0.13309985] [ 0.12548751]\n",
      "[ 0.66778922] [ 0.10562026] 205608\n",
      "[ 0.13847253] [ 0.12518272] 205608\n",
      "[0/25][150/1662] Loss_D: 0.133877 Loss_G: 0.384209 \n",
      "[ 0.62250805] [ 0.31090939]\n",
      "[ 0.12906873] [ 0.12092451]\n",
      "[ 0.70318526] [ 0.11377182] 205608\n",
      "[ 0.13373795] [ 0.11963334] 205608\n",
      "[0/25][155/1662] Loss_D: 0.121418 Loss_G: 0.388781 \n",
      "[ 0.64856368] [ 0.3415342]\n",
      "[ 0.12237327] [ 0.11474098]\n",
      "[ 0.73827279] [ 0.12219758] 205608\n",
      "[ 0.12966156] [ 0.11393048] 205608\n",
      "[0/25][160/1662] Loss_D: 0.104044 Loss_G: 0.394038 \n",
      "[ 0.69825917] [ 0.41084385]\n",
      "[ 0.11713158] [ 0.10896202]\n",
      "[ 0.77341378] [ 0.1305967] 205608\n",
      "[ 0.12523109] [ 0.10765678] 205608\n",
      "[0/25][165/1662] Loss_D: 0.101521 Loss_G: 0.399454 \n",
      "[ 0.74491912] [ 0.35113847]\n",
      "[ 0.11329326] [ 0.10300191]\n",
      "[ 0.80927765] [ 0.13861659] 205608\n",
      "[ 0.12060416] [ 0.101353] 205608\n",
      "[0/25][170/1662] Loss_D: 0.094774 Loss_G: 0.404039 \n",
      "[ 0.79060352] [ 0.38167405]\n",
      "[ 0.10962157] [ 0.09713744]\n",
      "[ 0.84471571] [ 0.14673485] 205608\n",
      "[ 0.11672066] [ 0.09564784] 205608\n",
      "[0/25][175/1662] Loss_D: 0.074694 Loss_G: 0.409726 \n",
      "[ 0.81393671] [ 0.40178657]\n",
      "[ 0.10317432] [ 0.09114384]\n",
      "[ 0.87991208] [ 0.15450624] 205608\n",
      "[ 0.11066482] [ 0.08973003] 205608\n",
      "[0/25][180/1662] Loss_D: 0.067103 Loss_G: 0.414616 \n",
      "[ 0.83297759] [ 0.45032302]\n",
      "[ 0.09622283] [ 0.08548791]\n",
      "[ 0.91353649] [ 0.16216129] 205608\n",
      "[ 0.10748521] [ 0.083901] 205608\n",
      "[0/25][185/1662] Loss_D: 0.058396 Loss_G: 0.419905 \n",
      "[ 0.82534927] [ 0.48124024]\n",
      "[ 0.09149989] [ 0.07875968]\n",
      "[ 0.94688535] [ 0.16996548] 205608\n",
      "[ 0.10300774] [ 0.07765388] 205608\n",
      "[0/25][190/1662] Loss_D: 0.051179 Loss_G: 0.424208 \n",
      "[ 0.87650907] [ 0.45057887]\n",
      "[ 0.08776626] [ 0.07479282]\n",
      "[ 0.97848928] [ 0.17732315] 205608\n",
      "[ 0.09993035] [ 0.07231097] 205608\n",
      "[0/25][195/1662] Loss_D: 0.043142 Loss_G: 0.428530 \n",
      "[ 0.90687913] [ 0.51011932]\n",
      "[ 0.08383802] [ 0.06918836]\n",
      "[ 1.00863826] [ 0.184617] 205608\n",
      "[ 0.09587486] [ 0.06729242] 205608\n",
      "[0/25][200/1662] Loss_D: 0.038555 Loss_G: 0.432313 \n",
      "[ 0.96749264] [ 0.52687746]\n",
      "[ 0.08446261] [ 0.0645671]\n",
      "[ 1.03840566] [ 0.19160572] 205608\n",
      "[ 0.09274372] [ 0.06282485] 205608\n",
      "[0/25][205/1662] Loss_D: 0.030061 Loss_G: 0.436641 \n",
      "[ 1.04197502] [ 0.56052214]\n",
      "[ 0.07384288] [ 0.06030905]\n",
      "[ 1.06581461] [ 0.19774294] 205608\n",
      "[ 0.08949514] [ 0.05785795] 205608\n",
      "[0/25][210/1662] Loss_D: 0.026662 Loss_G: 0.441526 \n",
      "[ 1.00654995] [ 0.59412044]\n",
      "[ 0.0675858] [ 0.05514409]\n",
      "[ 1.09142959] [ 0.20352814] 205608\n",
      "[ 0.08522703] [ 0.05274455] 205608\n",
      "[0/25][215/1662] Loss_D: 0.022781 Loss_G: 0.445371 \n",
      "[ 1.01414847] [ 0.5452432]\n",
      "[ 0.06914718] [ 0.04969714]\n",
      "[ 1.11482382] [ 0.20870075] 205608\n",
      "[ 0.08241457] [ 0.04728376] 205608\n",
      "[0/25][220/1662] Loss_D: 0.017691 Loss_G: 0.450105 \n",
      "[ 1.04810596] [ 0.58628249]\n",
      "[ 0.06148604] [ 0.04377664]\n",
      "[ 1.13730335] [ 0.21411777] 205608\n",
      "[ 0.07817886] [ 0.04254914] 205608\n",
      "[0/25][225/1662] Loss_D: 0.015521 Loss_G: 0.453993 \n",
      "[ 1.06522024] [ 0.50721323]\n",
      "[ 0.05856463] [ 0.03998219]\n",
      "[ 1.15674007] [ 0.21848685] 205608\n",
      "[ 0.07518398] [ 0.03781138] 205608\n",
      "[0/25][230/1662] Loss_D: 0.015849 Loss_G: 0.458961 \n",
      "[ 1.1126076] [ 0.53840643]\n",
      "[ 0.05777055] [ 0.03573915]\n",
      "[ 1.17551541] [ 0.22273298] 205608\n",
      "[ 0.0714656] [ 0.03372896] 205608\n",
      "[0/25][235/1662] Loss_D: 0.012660 Loss_G: 0.460976 \n",
      "[ 1.08166671] [ 0.56743431]\n",
      "[ 0.05192786] [ 0.03162338]\n",
      "[ 1.19282711] [ 0.22677681] 205608\n",
      "[ 0.0699399] [ 0.03001747] 205608\n",
      "[0/25][240/1662] Loss_D: 0.010718 Loss_G: 0.465350 \n",
      "[ 1.13784313] [ 0.6111849]\n",
      "[ 0.04370394] [ 0.02950945]\n",
      "[ 1.20783901] [ 0.23023173] 205608\n",
      "[ 0.06522753] [ 0.02677177] 205608\n",
      "[0/25][245/1662] Loss_D: 0.009456 Loss_G: 0.467903 \n",
      "[ 1.15957296] [ 0.6246438]\n",
      "[ 0.04240171] [ 0.02655689]\n",
      "[ 1.22046304] [ 0.23308033] 205608\n",
      "[ 0.06241021] [ 0.02351208] 205608\n",
      "[0/25][250/1662] Loss_D: 0.009944 Loss_G: 0.470146 \n",
      "[ 1.13895786] [ 0.6385234]\n",
      "[ 0.04714154] [ 0.02448497]\n",
      "[ 1.23265123] [ 0.23643072] 205608\n",
      "[ 0.06152858] [ 0.02145113] 205608\n",
      "[0/25][255/1662] Loss_D: 0.008717 Loss_G: 0.472348 \n",
      "[ 1.16341543] [ 0.634826]\n",
      "[ 0.04037751] [ 0.02116533]\n",
      "[ 1.24327767] [ 0.23940891] 205608\n",
      "[ 0.06029548] [ 0.01863436] 205608\n",
      "[0/25][260/1662] Loss_D: 0.007551 Loss_G: 0.474812 \n",
      "[ 1.15356004] [ 0.59167504]\n",
      "[ 0.04793336] [ 0.01819848]\n",
      "[ 1.25175059] [ 0.24171761] 205608\n",
      "[ 0.05851199] [ 0.01564661] 205608\n",
      "[0/25][265/1662] Loss_D: 0.005970 Loss_G: 0.477495 \n",
      "[ 1.16292465] [ 0.61314976]\n",
      "[ 0.04105925] [ 0.01537356]\n",
      "[ 1.25776958] [ 0.24352312] 205608\n",
      "[ 0.05629312] [ 0.01267525] 205608\n",
      "[0/25][270/1662] Loss_D: 0.005966 Loss_G: 0.479991 \n",
      "[ 1.16714668] [ 0.71741277]\n",
      "[ 0.03418198] [ 0.01215772]\n",
      "[ 1.2646898] [ 0.24559097] 205608\n",
      "[ 0.05587973] [ 0.01035122] 205608\n",
      "[0/25][275/1662] Loss_D: 0.008333 Loss_G: 0.481110 \n",
      "[ 1.16926837] [ 0.62698174]\n",
      "[ 0.03544995] [ 0.01183156]\n",
      "[ 1.26827013] [ 0.24660215] 205608\n",
      "[ 0.05566374] [ 0.00817526] 205608\n",
      "[0/25][280/1662] Loss_D: 0.006167 Loss_G: 0.482658 \n",
      "[ 1.18952334] [ 0.66584355]\n",
      "[ 0.03640584] [ 0.00908942]\n",
      "[ 1.2736969] [ 0.24805376] 205608\n",
      "[ 0.05569143] [ 0.00667439] 205608\n",
      "[0/25][285/1662] Loss_D: 0.006429 Loss_G: 0.485130 \n",
      "[ 1.16014433] [ 0.67804873]\n",
      "[ 0.03147019] [ 0.00749836]\n",
      "[ 1.27656674] [ 0.24786046] 205608\n",
      "[ 0.05392911] [ 0.00485614] 205608\n",
      "[0/25][290/1662] Loss_D: 0.005883 Loss_G: 0.487382 \n",
      "[ 1.17365491] [ 0.67650425]\n",
      "[ 0.03441279] [ 0.00493746]\n",
      "[ 1.2774322] [ 0.24823242] 205608\n",
      "[ 0.0514145] [ 0.00293174] 205608\n",
      "[0/25][295/1662] Loss_D: 0.008457 Loss_G: 0.487853 \n",
      "[ 1.24494898] [ 0.62749439]\n",
      "[ 0.02681712] [ 0.00418908]\n",
      "[ 1.27878904] [ 0.24955902] 205608\n",
      "[ 0.05248789] [ 0.00184727] 205608\n",
      "[0/25][300/1662] Loss_D: 0.006307 Loss_G: 0.488375 \n",
      "[ 1.20849741] [ 0.70309651]\n",
      "[ 0.03802475] [ 0.00427072]\n",
      "[ 1.28106201] [ 0.25053373] 205608\n",
      "[ 0.05467425] [ 0.00113078] 205608\n",
      "[0/25][305/1662] Loss_D: 0.005592 Loss_G: 0.489241 \n",
      "[ 1.2034297] [ 0.70555532]\n",
      "[ 0.03217434] [ 0.00262418]\n",
      "[ 1.28149855] [ 0.25045896] 205608\n",
      "[ 0.05380682] [ -5.95450401e-05] 205608\n",
      "[0/25][310/1662] Loss_D: 0.006480 Loss_G: 0.488883 \n",
      "[ 1.22680736] [ 0.67543668]\n",
      "[ 0.03091907] [ 0.00288495]\n",
      "[ 1.28316498] [ 0.25120878] 205608\n",
      "[ 0.05572806] [-0.00075306] 205608\n",
      "[0/25][315/1662] Loss_D: 0.006137 Loss_G: 0.490942 \n",
      "[ 1.2509774] [ 0.68430066]\n",
      "[ 0.02848442] [ 0.00013742]\n",
      "[ 1.28166187] [ 0.25118953] 205608\n",
      "[ 0.05422229] [-0.0020246] 205608\n",
      "[0/25][320/1662] Loss_D: 0.005136 Loss_G: 0.490603 \n",
      "[ 1.21726763] [ 0.64614695]\n",
      "[ 0.03006753] [  4.37349081e-05]\n",
      "[ 1.28253281] [ 0.25170729] 205608\n",
      "[ 0.05585312] [-0.00264627] 205608\n",
      "[0/25][325/1662] Loss_D: 0.005092 Loss_G: 0.492045 \n",
      "[ 1.21384478] [ 0.73276061]\n",
      "[ 0.03509041] [-0.00225299]\n",
      "[ 1.28222108] [ 0.25229296] 205608\n",
      "[ 0.05542961] [-0.00358652] 205608\n",
      "[0/25][330/1662] Loss_D: 0.006648 Loss_G: 0.491901 \n",
      "[ 1.2200079] [ 0.47873446]\n",
      "[ 0.03448272] [-0.00139527]\n",
      "[ 1.28243649] [ 0.25347742] 205608\n",
      "[ 0.05694283] [-0.0041571] 205608\n",
      "[0/25][335/1662] Loss_D: 0.006196 Loss_G: 0.490695 \n",
      "[ 1.24827647] [ 0.65926319]\n",
      "[ 0.04813045] [-0.00097129]\n",
      "[ 1.28278637] [ 0.25495207] 205608\n",
      "[ 0.05948229] [-0.00461421] 205608\n",
      "[0/25][340/1662] Loss_D: 0.006710 Loss_G: 0.491580 \n",
      "[ 1.2187103] [ 0.63120687]\n",
      "[ 0.03761204] [-0.00195722]\n",
      "[ 1.28354788] [ 0.25604522] 205608\n",
      "[ 0.06239635] [-0.00499944] 205608\n",
      "[0/25][345/1662] Loss_D: 0.005826 Loss_G: 0.492793 \n",
      "[ 1.21994066] [ 0.6240713]\n",
      "[ 0.03436247] [-0.00304688]\n",
      "[ 1.28239214] [ 0.25605884] 205608\n",
      "[ 0.0602437] [-0.00620112] 205608\n",
      "[0/25][350/1662] Loss_D: 0.005477 Loss_G: 0.492989 \n",
      "[ 1.21310508] [ 0.66342539]\n",
      "[ 0.0390836] [-0.00345979]\n",
      "[ 1.27986014] [ 0.25603968] 205608\n",
      "[ 0.06078279] [-0.0070817] 205608\n",
      "[0/25][355/1662] Loss_D: 0.005901 Loss_G: 0.491565 \n",
      "[ 1.19647717] [ 0.65725303]\n",
      "[ 0.0429946] [-0.00280983]\n",
      "[ 1.28093052] [ 0.25788835] 205608\n",
      "[ 0.06621817] [-0.00696795] 205608\n",
      "[0/25][360/1662] Loss_D: 0.004824 Loss_G: 0.493567 \n",
      "[ 1.1773684] [ 0.64674395]\n",
      "[ 0.02788872] [-0.00411838]\n",
      "[ 1.27786958] [ 0.25696462] 205608\n",
      "[ 0.06162509] [-0.00870163] 205608\n",
      "[0/25][365/1662] Loss_D: 0.005382 Loss_G: 0.495559 \n",
      "[ 1.1899159] [ 0.73484784]\n",
      "[ 0.03125385] [-0.00640723]\n",
      "[ 1.27594686] [ 0.25678697] 205608\n",
      "[ 0.06160942] [-0.0094975] 205608\n",
      "[0/25][370/1662] Loss_D: 0.004710 Loss_G: 0.495815 \n",
      "[ 1.18845284] [ 0.67546773]\n",
      "[ 0.03707901] [-0.00688569]\n",
      "[ 1.27256799] [ 0.25581732] 205608\n",
      "[ 0.06106561] [-0.01033925] 205608\n",
      "[0/25][375/1662] Loss_D: 0.004242 Loss_G: 0.494974 \n",
      "[ 1.15652943] [ 0.60802245]\n",
      "[ 0.0354215] [-0.00612612]\n",
      "[ 1.27324665] [ 0.25744137] 205608\n",
      "[ 0.06501514] [-0.01033506] 205608\n",
      "[0/25][380/1662] Loss_D: 0.004889 Loss_G: 0.496054 \n",
      "[ 1.19243157] [ 0.72186226]\n",
      "[ 0.02513245] [-0.00758298]\n",
      "[ 1.27023256] [ 0.25688618] 205608\n",
      "[ 0.06419733] [-0.01104969] 205608\n",
      "[0/25][385/1662] Loss_D: 0.005282 Loss_G: 0.496606 \n",
      "[ 1.19141233] [ 0.70172954]\n",
      "[ 0.01820945] [-0.00765821]\n",
      "[ 1.268332] [ 0.25671864] 205608\n",
      "[ 0.06496511] [-0.01162799] 205608\n",
      "[0/25][390/1662] Loss_D: 0.004591 Loss_G: 0.494577 \n",
      "[ 1.18885505] [ 0.70156133]\n",
      "[ 0.03859958] [-0.00899068]\n",
      "[ 1.26716161] [ 0.25745556] 205608\n",
      "[ 0.06737041] [-0.0120047] 205608\n",
      "[0/25][395/1662] Loss_D: 0.004598 Loss_G: 0.497612 \n",
      "[ 1.20343256] [ 0.73005658]\n",
      "[ 0.02759986] [-0.00953832]\n",
      "[ 1.26334548] [ 0.25789207] 205608\n",
      "[ 0.06616704] [-0.0127746] 205608\n",
      "[0/25][400/1662] Loss_D: 0.006125 Loss_G: 0.494164 \n",
      "[ 1.23786604] [ 0.72496659]\n",
      "[ 0.04038186] [-0.00787249]\n",
      "[ 1.26503527] [ 0.25989825] 205608\n",
      "[ 0.0730039] [-0.01240982] 205608\n",
      "[0/25][405/1662] Loss_D: 0.004209 Loss_G: 0.493674 \n",
      "[ 1.19522166] [ 0.67237228]\n",
      "[ 0.03920098] [-0.00898015]\n",
      "[ 1.26574349] [ 0.26055324] 205608\n",
      "[ 0.07486746] [-0.01301015] 205608\n",
      "[0/25][410/1662] Loss_D: 0.005235 Loss_G: 0.494688 \n",
      "[ 1.18423486] [ 0.63878042]\n",
      "[ 0.05149282] [-0.01060724]\n",
      "[ 1.26420963] [ 0.26108089] 205608\n",
      "[ 0.0763709] [-0.01363746] 205608\n",
      "[0/25][415/1662] Loss_D: 0.003956 Loss_G: 0.495176 \n",
      "[ 1.17759168] [ 0.59345669]\n",
      "[ 0.02978509] [-0.00782556]\n",
      "[ 1.26292324] [ 0.26168972] 205608\n",
      "[ 0.07604241] [-0.01460221] 205608\n",
      "[0/25][420/1662] Loss_D: 0.003854 Loss_G: 0.496068 \n",
      "[ 1.14498925] [ 0.67993706]\n",
      "[ 0.04431072] [-0.00876985]\n",
      "[ 1.2609756] [ 0.26166207] 205608\n",
      "[ 0.07630004] [-0.01533771] 205608\n",
      "[0/25][425/1662] Loss_D: 0.004253 Loss_G: 0.496950 \n",
      "[ 1.17481053] [ 0.76285553]\n",
      "[ 0.03425061] [-0.01431412]\n",
      "[ 1.257792] [ 0.26234508] 205608\n",
      "[ 0.07521272] [-0.01606502] 205608\n",
      "[0/25][430/1662] Loss_D: 0.003661 Loss_G: 0.499218 \n",
      "[ 1.22188687] [ 0.70661241]\n",
      "[ 0.03758858] [-0.01440684]\n",
      "[ 1.25693893] [ 0.26244119] 205608\n",
      "[ 0.07406221] [-0.01684503] 205608\n",
      "[0/25][435/1662] Loss_D: 0.004107 Loss_G: 0.496278 \n",
      "[ 1.21067941] [ 0.7751053]\n",
      "[ 0.04253027] [-0.01200967]\n",
      "[ 1.25763917] [ 0.26473728] 205608\n",
      "[ 0.0820349] [-0.01621516] 205608\n",
      "[0/25][440/1662] Loss_D: 0.005958 Loss_G: 0.498121 \n",
      "[ 1.21544468] [ 0.57438684]\n",
      "[ 0.02922393] [-0.01106395]\n",
      "[ 1.25053346] [ 0.26313287] 205608\n",
      "[ 0.07769672] [-0.0176574] 205608\n",
      "[0/25][445/1662] Loss_D: 0.003764 Loss_G: 0.493666 \n",
      "[ 1.16174424] [ 0.74356216]\n",
      "[ 0.04122626] [-0.01127189]\n",
      "[ 1.25335848] [ 0.26468012] 205608\n",
      "[ 0.08699443] [-0.01715456] 205608\n",
      "[0/25][450/1662] Loss_D: 0.004381 Loss_G: 0.495159 \n",
      "[ 1.17480993] [ 0.71709919]\n",
      "[ 0.03222992] [-0.01628747]\n",
      "[ 1.25207222] [ 0.26600963] 205608\n",
      "[ 0.09017133] [-0.01764419] 205608\n",
      "[0/25][455/1662] Loss_D: 0.004290 Loss_G: 0.494645 \n",
      "[ 1.15505016] [ 0.71248835]\n",
      "[ 0.04255725] [-0.01053903]\n",
      "[ 1.25288641] [ 0.26712811] 205608\n",
      "[ 0.09187335] [-0.01805391] 205608\n",
      "[0/25][460/1662] Loss_D: 0.004294 Loss_G: 0.498294 \n",
      "[ 1.15944004] [ 0.72203445]\n",
      "[ 0.03321549] [-0.01293495]\n",
      "[ 1.24726975] [ 0.26505142] 205608\n",
      "[ 0.084673] [-0.01978815] 205608\n",
      "[0/25][465/1662] Loss_D: 0.004427 Loss_G: 0.496163 \n",
      "[ 1.18228352] [ 0.73715752]\n",
      "[ 0.03635602] [-0.01264983]\n",
      "[ 1.24555862] [ 0.26649165] 205608\n",
      "[ 0.08739673] [-0.01960037] 205608\n",
      "[0/25][470/1662] Loss_D: 0.003922 Loss_G: 0.497943 \n",
      "[ 1.19287479] [ 0.74483901]\n",
      "[ 0.03176121] [-0.01188858]\n",
      "[ 1.24367213] [ 0.2665332] 205608\n",
      "[ 0.08886992] [-0.020607] 205608\n",
      "[0/25][475/1662] Loss_D: 0.004127 Loss_G: 0.498705 \n",
      "[ 1.17494643] [ 0.70798421]\n",
      "[ 0.03711116] [-0.01684614]\n",
      "[ 1.24000359] [ 0.26678103] 205608\n",
      "[ 0.0863152] [-0.02136664] 205608\n",
      "[0/25][480/1662] Loss_D: 0.004333 Loss_G: 0.496817 \n",
      "[ 1.1505605] [ 0.67061031]\n",
      "[ 0.03983919] [-0.01357871]\n",
      "[ 1.23885465] [ 0.26829594] 205608\n",
      "[ 0.09351546] [-0.02084514] 205608\n",
      "[0/25][485/1662] Loss_D: 0.004105 Loss_G: 0.495770 \n",
      "[ 1.1503551] [ 0.71698391]\n",
      "[ 0.07002107] [-0.0169005]\n",
      "[ 1.23727727] [ 0.27037299] 205608\n",
      "[ 0.09763557] [-0.02115778] 205608\n",
      "[0/25][490/1662] Loss_D: 0.004212 Loss_G: 0.499003 \n",
      "[ 1.19351482] [ 0.7511903]\n",
      "[ 0.0326786] [-0.01448867]\n",
      "[ 1.23576629] [ 0.2690188] 205608\n",
      "[ 0.09321824] [-0.02276281] 205608\n",
      "[0/25][495/1662] Loss_D: 0.003805 Loss_G: 0.498545 \n",
      "[ 1.1907109] [ 0.76951587]\n",
      "[ 0.04497369] [-0.01889255]\n",
      "[ 1.23323584] [ 0.27027324] 205608\n",
      "[ 0.09501094] [-0.02258342] 205608\n",
      "[0/25][500/1662] Loss_D: 0.003684 Loss_G: 0.499050 \n",
      "[ 1.17626917] [ 0.6920566]\n",
      "[ 0.0326697] [-0.01545884]\n",
      "[ 1.23276329] [ 0.27224615] 205608\n",
      "[ 0.0958293] [-0.02336946] 205608\n",
      "[0/25][505/1662] Loss_D: 0.004058 Loss_G: 0.497396 \n",
      "[ 1.15125012] [ 0.74996841]\n",
      "[ 0.04782218] [-0.01624636]\n",
      "[ 1.23487508] [ 0.27573869] 205608\n",
      "[ 0.10178707] [-0.02281264] 205608\n",
      "[0/25][510/1662] Loss_D: 0.003752 Loss_G: 0.500418 \n",
      "[ 1.21501601] [ 0.54173601]\n",
      "[ 0.0309616] [-0.01849482]\n",
      "[ 1.22898424] [ 0.27421948] 205608\n",
      "[ 0.09235277] [-0.02534287] 205608\n",
      "[0/25][515/1662] Loss_D: 0.003878 Loss_G: 0.499116 \n",
      "[ 1.16902554] [ 0.74247688]\n",
      "[ 0.0420393] [-0.01686069]\n",
      "[ 1.22783291] [ 0.2755698] 205608\n",
      "[ 0.10010266] [-0.02467587] 205608\n",
      "[0/25][520/1662] Loss_D: 0.003839 Loss_G: 0.498358 \n",
      "[ 1.16443121] [ 0.74682307]\n",
      "[ 0.03757711] [-0.01849715]\n",
      "[ 1.22775912] [ 0.27585956] 205608\n",
      "[ 0.10060668] [-0.02518748] 205608\n",
      "[0/25][525/1662] Loss_D: 0.003113 Loss_G: 0.498657 \n",
      "[ 1.15743232] [ 0.80276632]\n",
      "[ 0.03213187] [-0.01392403]\n",
      "[ 1.2257061] [ 0.27601764] 205608\n",
      "[ 0.10052285] [-0.02533655] 205608\n",
      "[0/25][530/1662] Loss_D: 0.004097 Loss_G: 0.493935 \n",
      "[ 1.14622641] [ 0.66324151]\n",
      "[ 0.06738428] [-0.01668363]\n",
      "[ 1.22919488] [ 0.27927798] 205608\n",
      "[ 0.11432933] [-0.02403191] 205608\n",
      "[0/25][535/1662] Loss_D: 0.004258 Loss_G: 0.496263 \n",
      "[ 1.15860581] [ 0.59349918]\n",
      "[ 0.05020526] [-0.01655275]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-10:\n",
      "Process Process-9:\n",
      "Process Process-7:\n",
      "Process Process-8:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2646a778c2cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mpred_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnetD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0mpred_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnetD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2646a778c2cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mneg_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_dataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0md_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m  \u001b[1;31m# ensure that the worker exits on process exit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/process.pyc\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mforking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0m_current_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/forking.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m    124\u001b[0m                     \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                     \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m                 \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bootstrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m                 \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/multiprocessing/process.pyc\u001b[0m in \u001b[0;36m_bootstrap\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Process %s:\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m             \u001b[0mtraceback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'process exiting with exitcode %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mexitcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/traceback.pyc\u001b[0m in \u001b[0;36mprint_exc\u001b[1;34m(limit, file)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m         \u001b[0mprint_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \u001b[0metype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/traceback.pyc\u001b[0m in \u001b[0;36mprint_exception\u001b[1;34m(etype, value, tb, limit, file)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0m_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Traceback (most recent call last):'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0mprint_tb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_exception_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/traceback.pyc\u001b[0m in \u001b[0;36mprint_tb\u001b[1;34m(tb, limit, file)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mco\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         _print(file,\n\u001b[1;32m---> 67\u001b[1;33m                '  File \"%s\", line %d, in %s' % (filename, lineno, name))\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlineno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    data_iter = iter(dataloader)\n",
    "    neg_iter = iter(neg_dataloader)\n",
    "    d_step = 5\n",
    "    i = 0\n",
    "    while i< len(dataloader):\n",
    "        j = 0\n",
    "\n",
    "        while j<d_step and i < len(dataloader):\n",
    "            ############################\n",
    "            # (1) Update D network\n",
    "            ###########################\n",
    "            j += 1\n",
    "            i += 1\n",
    "            data = data_iter.next()\n",
    "\n",
    "            # sample data with real and fake\n",
    "            real_cpu = data\n",
    "\n",
    "            if opt.cuda:\n",
    "                real_cpu = real_cpu.cuda()\n",
    "            input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "            inputv = Variable(input)\n",
    "            try:\n",
    "                noise = neg_iter.next()\n",
    "    #                 print (noise.size())\n",
    "            except:\n",
    "                neg_iter = iter(neg_dataloader)\n",
    "                noise = neg_iter.next()\n",
    "            if opt.cuda:\n",
    "                noise = noise.cuda()\n",
    "            # noise.resize_(opt.batchSize, nz, 1, 1).normal_(0, 1)\n",
    "            noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "\n",
    "            # Discriminator\n",
    "            D_real = netD(inputv)\n",
    "            D_fake = netD(fake)\n",
    "\n",
    "            errD = 0.5 * (torch.mean((D_real - 1)**2) + torch.mean(D_fake**2))\n",
    "            errD.backward()\n",
    "            optimizerD.step()\n",
    "            reset_grad()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network\n",
    "            ###########################\n",
    "        try:\n",
    "                noise = neg_iter.next()\n",
    "\n",
    "        except:\n",
    "                neg_iter = iter(neg_dataloader)\n",
    "                noise = neg_iter.next()\n",
    "        if opt.cuda:\n",
    "            noise = noise.cuda()\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        G_fake = netD(fake)\n",
    "        errG = 0.5 * torch.mean((G_fake - 1)**2)\n",
    "\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        reset_grad()\n",
    "        print('[%d/%d][%d/%d] Loss_D: %f Loss_G: %f '\n",
    "            % (epoch, opt.niter, i, len(dataloader),\n",
    "            errD.data[0], errG.data[0]))\n",
    "        pred_probs = (D_real.cpu().data.numpy())\n",
    "        print (max(pred_probs),min(pred_probs))\n",
    "        pred_probs = (G_fake.cpu().data.numpy())\n",
    "        print (max(pred_probs),min(pred_probs))\n",
    "\n",
    "        pred_probs = (netD(testv).cpu().data.numpy())\n",
    "        print (max(pred_probs),min(pred_probs),len(pred_probs))\n",
    "        pred_probs = (netD(netG(testv)).cpu().data.numpy())\n",
    "        print (max(pred_probs),min(pred_probs),len(pred_probs))\n",
    "    torch.save(netD.state_dict(), '{0}/lsgan_netD_epoch_{1}.pth'.format(opt.experiment, epoch))\n",
    "\n",
    "#         pred_probs = (netD(testv.cuda()).cpu().data.numpy())\n",
    "#         pred_probs = (pred_probs-min(pred_probs))/(max(pred_probs)-min(pred_probs))\n",
    "#         for i in range(0,10,2):\n",
    "#             pred = [1 if j>i/10.0 else 0 for j in pred_probs ]\n",
    "#             print (confusion_matrix(labels,pred))\n",
    "#             print (\"Accuracy, \",  metrics.accuracy_score(labels,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_probs = (netD((testv)).cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.22585964], dtype=float32), array([ 0.2777372], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(pred_probs),min(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    190882\n",
       "0     14726\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1 if j<=-0.009461942 else 0 for j in pred_probs ]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 38823  14335]\n",
      " [124508  27942]]\n",
      "Accuracy,  0.324719855259\n"
     ]
    }
   ],
   "source": [
    "pred_probs = (pred_probs-min(pred_probs))/(max(pred_probs)-min(pred_probs))\n",
    "print (confusion_matrix(labels,pred))\n",
    "print (\"Accuracy, \",  metrics.accuracy_score(labels,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 53158      0]\n",
      " [152449      1]]\n",
      "Accuracy,  0.258545387339\n",
      "[[ 53146     12]\n",
      " [152321    129]]\n",
      "Accuracy,  0.259109567721\n",
      "[[ 52722    436]\n",
      " [148685   3765]]\n",
      "Accuracy,  0.274731527956\n",
      "[[ 46820   6338]\n",
      " [118010  34440]]\n",
      "Accuracy,  0.395218084899\n",
      "[[ 14335  38823]\n",
      " [ 27942 124508]]\n",
      "Accuracy,  0.675280144741\n"
     ]
    }
   ],
   "source": [
    "pred_probs = (pred_probs-min(pred_probs))/(max(pred_probs)-min(pred_probs))\n",
    "for i in range(0,10,2):\n",
    "    pred = [0 if j>i/10.0 else 1 for j in pred_probs ]\n",
    "    print (confusion_matrix(labels,pred))\n",
    "    print (\"Accuracy, \",  metrics.accuracy_score(labels,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    163331\n",
       "0     42277\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([0 if j > 0.8 else 1 for j in pred_probs ]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    152450\n",
       "0     53158\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.30000000e+01,   1.28000000e+02,   8.26000000e+02,\n",
       "          3.23400000e+03,   9.72800000e+03,   2.68490000e+04,\n",
       "          5.24240000e+04,   7.01290000e+04,   3.99140000e+04,\n",
       "          2.36300000e+03]),\n",
       " array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAFkCAYAAACAUFlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJEmSJEmSlMj/B1pQd41ei09wAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd15f20f890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.use('Agg')\n",
    "plt.hist(pred_probs,bins=[i/10.0for i in range(0,11,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdataloader = torch.utils.data.DataLoader(test, batch_size=len(test),\n",
    "                                         shuffle=False, num_workers=int(opt.workers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdataiter = iter(testdataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testv = Variable(testdataiter.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([205608, 148])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testv.data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netP = mlp.MLP_P(opt.nSize,  ndf, ngpu)\n",
    "netP.load_state_dict(torch.load('{0}/netD_epoch_{1}.pth'.format(opt.experiment, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_probs = (netP(netG(testv)).data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.], dtype=float32), array([ 0.], dtype=float32))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(pred_probs),min(pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    pred_probs = (pred_probs-min(pred_probs))/(max(pred_probs)-min(pred_probs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0  53158]\n",
      " [     1 152449]]\n",
      "Accuracy,  0.741454612661\n",
      "[[    10  53148]\n",
      " [    29 152421]]\n",
      "Accuracy,  0.741367067429\n",
      "[[   193  52965]\n",
      " [   598 151852]]\n",
      "Accuracy,  0.739489708572\n",
      "[[  1693  51465]\n",
      " [  4824 147626]]\n",
      "Accuracy,  0.726231469593\n",
      "[[  7591  45567]\n",
      " [ 19759 132691]]\n",
      "Accuracy,  0.682278899654\n",
      "[[20295 32863]\n",
      " [52989 99461]]\n",
      "Accuracy,  0.582448153768\n",
      "[[ 37613  15545]\n",
      " [101433  51017]]\n",
      "Accuracy,  0.431062993658\n",
      "[[ 49490   3668]\n",
      " [138630  13820]]\n",
      "Accuracy,  0.307916034396\n",
      "[[ 52770    388]\n",
      " [150805   1645]]\n",
      "Accuracy,  0.264654099062\n",
      "[[ 53144     14]\n",
      " [152369     81]]\n",
      "Accuracy,  0.258866386522\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10,1):\n",
    "    pred = [1 if j>i/10.0 else 0 for j in pred_probs ]\n",
    "    print (confusion_matrix(labels,pred))\n",
    "    print (\"Accuracy, \",  metrics.accuracy_score(labels,pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 53158      0]\n",
      " [152449      1]]\n",
      "Accuracy,  0.258545387339\n",
      "[[ 33422  19736]\n",
      " [105121  47329]]\n",
      "Accuracy,  0.392742500292\n",
      "[[20374 32784]\n",
      " [66066 86384]]\n",
      "Accuracy,  0.519230769231\n",
      "[[  9657  43501]\n",
      " [ 33499 118951]]\n",
      "Accuracy,  0.62550095327\n",
      "[[  3441  49717]\n",
      " [ 13522 138928]]\n",
      "Accuracy,  0.692429282907\n",
      "[[   945  52213]\n",
      " [  4450 148000]]\n",
      "Accuracy,  0.724412474223\n",
      "[[   201  52957]\n",
      " [  1237 151213]]\n",
      "Accuracy,  0.736420761838\n",
      "[[    42  53116]\n",
      " [   271 152179]]\n",
      "Accuracy,  0.740345706393\n",
      "[[     8  53150]\n",
      " [    49 152401]]\n",
      "Accuracy,  0.741260067702\n",
      "[[     1  53157]\n",
      " [     5 152445]]\n",
      "Accuracy,  0.741440021789\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10,1):\n",
    "    pred = [0 if j>i/10.0 else 1 for j in pred_probs ]\n",
    "    print (confusion_matrix(labels,pred))\n",
    "    print (\"Accuracy, \",  metrics.accuracy_score(labels,pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
